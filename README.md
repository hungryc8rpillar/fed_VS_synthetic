# fed_VS_synthetic
This notebook is work in progress and requires heavy refactoring before going into deployment. The chosen hyperparameters and demonstrated results are examples and not representative for the potential of the investigated methods. The sole purpose of this notebook is showing an interested reader, what I am working on within the frame of my thesis. The medical data I am building this application for is confidential, thus this version uses the public MNIST set.

## Motivation
The CellFace project is located in Munich and Singapore. With GDPR (Munich) and PDPA (Singapore) different data protection laws apply.  International data transfer might be challenging and requires a responsible approach aligned to regulations. In addition, the majority of the data we deal with in CellFace is sensitive as it originates from clinical patients. To make proper use of this data in model training without risking privacy breaches, two approaches shall be investigated and compared to a benchmark. Federated Learning is the method of training the same model in parallel, directly on the devices the data is stored on. After completion of a training cycle on several devices the global model parameters are defined by an aggregation over the local training results. No sensitive information has to leave its origin storage as the model comes to the data instead of the other way round. Synthetic data is the practice of enabling generative models to create from one dataset a new, artificial one that keeps the original statistical properties but preserves privacy about personal data. Ideally this synthetic dataset is then anonymous and can be shared without risking privacy breaches and be used for model training. As in practice both approaches satisfy some privacy guarantees but are not immune to attacks, the influence of adding differential privacy on both methods is also subject for investigation.
